# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q2bs11ydJjmyOyHw0H9bTz0LdcXdVZ7X
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
import time


def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df


class Classification():
    def __init__(self,model_type,x_train,y_train,x_val,y_val,hyper_tuning_method = "grid"):
          self.model_type = model_type
          self.x_train = x_train
          self.y_train = y_train
          self.x_val = x_val
          self.y_val = y_val
          self.hyper_tuning_method = hyper_tuning_method
          
          self.feature_importances = pd.DataFrame()

          if self.model_type == "Logistic Regression":
              self.model = LogisticRegression(fit_intercept=False)
          elif self.model_type == 'Decision Tree':
              from sklearn.tree import DecisionTreeClassifier
              self.model = DecisionTreeClassifier(random_state=42)
          elif self.model_type == 'Random Forest':
              self.model = RandomForestClassifier(n_estimators=20,n_jobs=-1,random_state=42)
          elif self.model_type == 'SVM':
            self.model = SVC()
          elif self.model_type == 'KNN':
            self.model = KNeighborsClassifier(n_jobs=-1)

    def score(self,best_model):
        self.scores_table = pd.DataFrame()
        self.acc_train = self.best_model.score(self.x_train,self.y_train)
        self.acc_val = self.best_model.score(self.x_val,self.y_val)
        d = {'Model Name': [self.model_type],
             'Train Accuracy': [self.acc_train], 
             'Validation Accuracy': [self.acc_val],
             'Accuracy Difference':[self.acc_train-self.acc_val]}
        self.scores_table = pd.DataFrame(data=d)
        return self.scores_table

    def get_best_model(self):
        return self.best_model

    def get_scores(self,params,cv_type=5):
        start = time.time()
        classifier = self.model
        if self.hyper_tuning_method == "grid":
            tuning_model = GridSearchCV(estimator = classifier,
                                  param_grid = params,
                                  scoring = 'accuracy', 
                                  cv = cv_type, 
                                  return_train_score=True,
                                  n_jobs = -1)
        else:
            tuning_model = RandomizedSearchCV(classifier,
                                  params,
                                  scoring = 'accuracy', 
                                  cv = cv_type, 
                                  return_train_score=True,
                                  n_jobs = -1,
                                  n_iter=60)
            
        self.tuning_model = tuning_model.fit(self.x_train,self.y_train)
        self.best_model = tuning_model.best_estimator_
        self.best_params =tuning_model.best_params_
        
        self.score = self.score(self.best_model)
        display(self.scores_table)
        print()
        if params == {}:
            pass
        else:
            print("The best hyperparameters by {}searchCv are : ".format(self.hyper_tuning_method), self.best_params,'\n')
        self.y_validated = self.best_model.predict(self.x_val)
        self.classification_report = pd.DataFrame.from_dict(classification_report(self.y_val,self.y_validated,output_dict=True)).iloc[0:3,0:5]
        stop = time.time()
        print(f"time taken by hayper perameter for searching best perameter : {stop - start}s")
        return self.classification_report
  
    def get_feature_importances(self):
 
        if (self.model_type == 'Decision Tree') or (self.model_type == 'Random Forest') or (self.model_type == 'SVM'):    
            self.feature_importances_table = pd.DataFrame(self.best_model.feature_importances_,
                                                    index = self.x_train.columns,
                                                    columns=['Importance']).sort_values('Importance',ascending =False)
            plt.figure(figsize=(9,7.5))
            self.feature_importances_bar = sns.barplot(y= self.feature_importances_table.index[:15], x= self.feature_importances_table['Importance'][:15])
            plt.show()
            return self.feature_importances_bar  
        else:
            return print('This classification method does not have the attribute feature importance.')    

    def confusion_matrix(self):
        plt.figure(figsize=(9,9))
        ax = sns.heatmap(confusion_matrix(self.y_val, self.y_validated),
                         annot= True, 
                         fmt = '.4g', 
                         cbar=0,
                         xticklabels=[1,2,3,4,5],
                         yticklabels=[1,2,3,4,5])
        ax.set(xlabel='Predicted', ylabel='True')
        plt.show()
         
    def get_test_score(self,x_test,y_test):
        self.y_test = x_test
        self.x_test = x_test
        self.y_tested = self.best_model.predict(self.x_test)
        self.test_classification_report = pd.DataFrame.from_dict(classification_report(self.y_test,self.y_tested,output_dict=True)).iloc[0:3,0:5]
        
        return self.test_scores

    def test_confusion_metrix(self):
        plt.figure(figsize=(9,9))
        ax = sns.heatmap(confusion_matrix(self.y_test, self.y_tested),
                         annot= True, 
                         fmt = '.4g', 
                         cbar=0,
                         xticklabels=[1,2,3,4,5],
                         yticklabels=[1,2,3,4,5])
        ax.set(xlabel='Predicted', ylabel='True')
        plt.show()

   
   
   

            
        

    def test_roc_curve(self):
        ns_auc = roc_auc_score(self.y_test,self.y_tested)
        print('Logistic: ROC AUC=%.3f' % (lr_auc))

    def roc_curve(self):
        ns_auc = roc_auc_score(self.y_train,self.y_validated)
        print('Logistic: ROC AUC=%.3f' % (lr_auc))

